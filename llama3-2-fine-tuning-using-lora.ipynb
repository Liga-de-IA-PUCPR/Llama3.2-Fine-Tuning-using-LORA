{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"096ba04a1d7f45ec84f06a7a9f8276b7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"251ff2c9141c4da99f51008dfbee9b04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_096ba04a1d7f45ec84f06a7a9f8276b7","placeholder":"​","style":"IPY_MODEL_d4a6149f85a14809b2ec8162722a654c","value":" 2/2 [00:12&lt;00:00,  5.40s/it]"}},"528923440b3a482c9dc8c04157c687fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7086cff6a30948e5b4af029e653f8e71","placeholder":"​","style":"IPY_MODEL_644476f799be4a3f9354e3e5c47b6feb","value":"Loading checkpoint shards: 100%"}},"5cfc4ffce7934bc983e9d1bd43b388b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_528923440b3a482c9dc8c04157c687fa","IPY_MODEL_cb6170e69e4349c792f640fab25f8cf6","IPY_MODEL_251ff2c9141c4da99f51008dfbee9b04"],"layout":"IPY_MODEL_ece5939e218d4acca4aacf3e0ca6f947"}},"644476f799be4a3f9354e3e5c47b6feb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7086cff6a30948e5b4af029e653f8e71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b21e3f35d647483b8c11937e9f715702":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb6170e69e4349c792f640fab25f8cf6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d008a6ef62db4bfb9e241c35ace92811","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b21e3f35d647483b8c11937e9f715702","value":2}},"d008a6ef62db4bfb9e241c35ace92811":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4a6149f85a14809b2ec8162722a654c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ece5939e218d4acca4aacf3e0ca6f947":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":9899965,"sourceType":"datasetVersion","datasetId":6081302},{"sourceId":118664,"sourceType":"modelInstanceVersion","modelInstanceId":99804,"modelId":123977},{"sourceId":120002,"sourceType":"modelInstanceVersion","modelInstanceId":100933,"modelId":121027}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Table of Contents</span></b> <a class='anchor' id='top'></a>\n<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<li> <a href=\"#install_libraries\">Install libraries</a></li>\n<li><a href=\"#import_libraries\">Import Libraries</a></li>\n<li><a href=\"#load_data\">Load Data</a></li>\n<li><a href=\"#configuration\">Configuration</a></li>\n<li><a href=\"#configure_parameters\">Configure Quantization and LORA-specific parameters</a></li>\n<li><a href=\"#load_model\">Load Model</a></li>\n<li><a href=\"#training\">Training</a></li>\n<li><a href=\"#testing\">Testing</a></li>\n<li><a href=\"#save_model\">Saving model for inference</a></li>\n</div>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Install Libraries</b><a class='anchor' id='install_libraries'></a> [↑](#top) \n\n***\n\nInstall all the required libraries for this notebook.","metadata":{}},{"cell_type":"code","source":"# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.33.1 trl==0.4.7\n# !pip install accelerate peft==0.4.0 bitsandbytes transformers==4.33.1 trl==0.4.7\n# !pip install -q bitsandbytes==0.41.1 transformers==4.38.2 accelerate==0.21.0 peft==0.4.0 trl==0.4.7\n# !pip install -q bitsandbytes transformers accelerate peft trl\n# !pip install bitsandbytes>=0.43.0 transformers>=4.43.0 accelerate>=0.28.0 peft>=0.5.0 trl>=0.7.11\n!pip install -q --upgrade transformers==4.43.0\n!pip install -q --upgrade bitsandbytes accelerate peft trl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:43:33.324542Z","iopub.execute_input":"2024-12-05T00:43:33.324869Z","iopub.status.idle":"2024-12-05T00:44:30.125612Z","shell.execute_reply.started":"2024-12-05T00:43:33.324842Z","shell.execute_reply":"2024-12-05T00:44:30.124691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mostra as versões das bibliotecas\nimport importlib\n\n# Lista das bibliotecas a verificar\nlibraries = [\"bitsandbytes\", \"transformers\", \"accelerate\", \"peft\", \"trl\"]\n\n# Itera pelas bibliotecas e exibe suas versões\nfor lib in libraries:\n    try:\n        module = importlib.import_module(lib)\n        version = getattr(module, \"__version__\", \"Versão desconhecida\")\n        print(f\"{lib}: {version}\")\n    except ImportError:\n        print(f\"{lib}: Não instalada\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:44:30.127255Z","iopub.execute_input":"2024-12-05T00:44:30.127519Z","iopub.status.idle":"2024-12-05T00:44:44.473782Z","shell.execute_reply.started":"2024-12-05T00:44:30.127489Z","shell.execute_reply":"2024-12-05T00:44:44.472924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:44:44.475085Z","iopub.execute_input":"2024-12-05T00:44:44.475764Z","iopub.status.idle":"2024-12-05T00:44:44.496482Z","shell.execute_reply.started":"2024-12-05T00:44:44.475719Z","shell.execute_reply":"2024-12-05T00:44:44.495585Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Import Libraries</b><a class='anchor' id='import_libraries'></a> [↑](#top) \n\n***\n\nImport all the required libraries for this notebook.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport os\nimport torch\n# import cuda\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoModel,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"id":"GCWf-ANoqlgr","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:44:44.498955Z","iopub.execute_input":"2024-12-05T00:44:44.499334Z","iopub.status.idle":"2024-12-05T00:44:45.919920Z","shell.execute_reply.started":"2024-12-05T00:44:44.499304Z","shell.execute_reply":"2024-12-05T00:44:45.919284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Load custom dataset</b><a class='anchor' id='load_data'></a> [↑](#top) \n\n***\n\nCustom dataset is used in this notebook. You can use any data but dataset should contain two columns with name 'prompt' and 'response'. The prompt column should contain the input text.","metadata":{}},{"cell_type":"code","source":"import random\ndf = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\")\ndf_mis_map = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n\n# Prefixos para construir as colunas de prompt e response\nquestion_prefix = \"Question:\"\nincorrect_answer_prefix = \"Incorrect Answer:\"\ncorrect_answer_prefix = \"Correct Answer:\"\nresponse_start = \"Misconception for incorrect answer:\"\n\ndef simular_respostas(df):\n    prompts = []\n    responses = []\n    \n    # Iterar pelas linhas do DataFrame\n    for _, row in df.iterrows():\n        question = f\"{question_prefix} {row['QuestionText']}\"\n        correct_answer = row['CorrectAnswer']\n        \n        # Obter todas as opções disponíveis\n        options = ['AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText']\n        misconception_columns = ['MisconceptionAId', 'MisconceptionBId', 'MisconceptionCId', 'MisconceptionDId']\n        \n        # Simular a resposta do indivíduo (75% de chance de errar para dar mais foco nos erros)\n        if random.random() < 0.75:  # Simular erro\n            incorrect_options = [opt for opt in options if row[opt] != correct_answer]\n            chosen_option = random.choice(incorrect_options)talling all packages, you \n            chosen_misconception = misconception_columns[options.index(chosen_option)]\n            \n            # Criar prompt e response para a resposta errada\n            prompt = f\"{question}\\n\\n{incorrect_answer_prefix} {row[chosen_option]}\\n\\n{correct_answer_prefix} {correct_answer}\"\n            \n            if not pd.isna(row[chosen_misconception]):  # Misconception associada\n                response = f\"{response_start} Misconception ID {int(row[chosen_misconception])}\"\n            else:  # Sem misconception associada\n                response = f\"{response_start} No specific misconception mapped.\"\n        else:  # Simular acerto\n            chosen_option = correct_answer\n            prompt = f\"{question}\\n\\n{correct_answer_prefix} {chosen_option}\"\n            response = \"Correct answer. No misconception.\"\n\n        # Adicionar ao DataFrame final\n        prompts.append(prompt)\n        responses.append(response)\n    \n    # Criar o DataFrame final\n    df_final = pd.DataFrame({'prompt': prompts, 'response': responses})\n    return df_final\n\n# Gerar a base simulada\ndf_simulado = simular_respostas(df)\n\n# Visualizar a base gerada\ndf = df_simulado","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:44:45.920860Z","iopub.execute_input":"2024-12-05T00:44:45.921092Z","iopub.status.idle":"2024-12-05T00:44:46.091817Z","shell.execute_reply.started":"2024-12-05T00:44:45.921071Z","shell.execute_reply":"2024-12-05T00:44:46.090878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DATASET DE TESTE\n# Load and display the first few rows of the dataset\n# df = pd.read_csv(\"/content/drive/MyDrive/task2_10k.csv\")\n# df = pd.read_csv(\"/kaggle/input/test-dataset/dataset_baixado.csv\")\n# df = df.drop(['product', 'category'], axis=1)\n# df = df.rename(columns={'description': 'prompt'})\n# df = df.rename(columns={'text': 'response'})\n\n# df.head()talling all packages, you ","metadata":{"id":"sYsGSVRMrBWR","outputId":"c70dde02-8fd7-47c5-9b5f-ef61e8ce615c","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:44:46.092871Z","iopub.execute_input":"2024-12-05T00:44:46.093117Z","iopub.status.idle":"2024-12-05T00:44:46.096997Z","shell.execute_reply.started":"2024-12-05T00:44:46.093096Z","shell.execute_reply":"2024-12-05T00:44:46.096146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess the dataset by removing hyperlinks and mentions\nfor i in range(len(df)):\n    l = df['response'][i]\n    text = l.replace(\"<hyperlink>\",\"\")\n    l = text.replace(\"<mention>\",\"\")\n    df['response'][i] = l","metadata":{"id":"UwzKfUCePBCU","outputId":"07246e64-2afe-421e-afef-43852ee26ac3","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:44:46.097958Z","iopub.execute_input":"2024-12-05T00:44:46.098213Z","iopub.status.idle":"2024-12-05T00:44:46.261483Z","shell.execute_reply.started":"2024-12-05T00:44:46.098182Z","shell.execute_reply":"2024-12-05T00:44:46.260445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the dataset into final test data and remaining data\nfinal_test_data = df[8000:10000]\ndf = df.drop(final_test_data.index)","metadata":{"id":"veYfw2QTPlkw","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:44:46.262644Z","iopub.execute_input":"2024-12-05T00:44:46.262906Z","iopub.status.idle":"2024-12-05T00:44:46.269309Z","shell.execute_reply.started":"2024-12-05T00:44:46.262883Z","shell.execute_reply":"2024-12-05T00:44:46.268557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Here test set is the validation set\n# Split the data into train and test sets, with 90% in the train set\ntrain_df = df.sample(frac=0.9, random_state=42)\ntest_df = df.drop(train_df.index)\n\n# Save the dataframes to .jsonl files\ntrain_df.to_json('train.jsonl', orient='records', lines=True)\ntest_df.to_json('test.jsonl', orient='records', lines=True)","metadata":{"id":"ogmuoMFWq_4q","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:44:46.270804Z","iopub.execute_input":"2024-12-05T00:44:46.271344Z","iopub.status.idle":"2024-12-05T00:44:46.291232Z","shell.execute_reply.started":"2024-12-05T00:44:46.271319Z","shell.execute_reply":"2024-12-05T00:44:46.290611Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Configuration</b><a class='anchor' id='configuration'></a> [↑](#top) \n\n***\n\nCentral repository for this notebook's hyperparameters.","metadata":{}},{"cell_type":"code","source":"# Set up model configuration and training parameters\n# model_name = \"NousResearch/llama-2-7b-chat-hf\"\nmodel_name = \"/kaggle/input/llama-3.2/transformers/1b-instruct/1\"\n# model_name = \"meta-llama/Llama-3.2-1B\" # teste\n# model_name = \"meta-llama/Llama-Guard-3-1B\" # Teste\n# model_name = \"NousResearch/Llama-3.2-1B\" # Teste\n# model_name = \"meta-llama/Llama-3.2-1B\" # Teste\ndataset_name = \"/content/train.jsonl\"\nnew_model = \"llama-3.2-1b-eedi_misconceptions\"\n# Ver na liga quais parametros do LoRA a gente pode alterar para ter um melhor desempenho\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\noutput_dir = \"./results\"\nnum_train_epochs = 10\nfp16 = False #False\nbf16 = False\nper_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 5\nmax_seq_length = None\npacking = False\n# device_map = {\"cpu\": 0}\n# device_map = {\"cuda\": 0}\n# device_map = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice_map = \"auto\" #\"cuda:0\" #TIVE QUE TROCAR PARA AUTO, POIS NÂO ESTAVA FUNCIONANDO O PARALELISMO DO CUDA, AINDA NÂO SEI PQ!\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device_map)\nprint(device)","metadata":{"id":"bqfbhUZI-4c_","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:44:46.293556Z","iopub.execute_input":"2024-12-05T00:44:46.293802Z","iopub.status.idle":"2024-12-05T00:44:46.301026Z","shell.execute_reply.started":"2024-12-05T00:44:46.293781Z","shell.execute_reply":"2024-12-05T00:44:46.300168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = load_dataset('json', data_files='/kaggle/working/train.jsonl', split=\"train\")\nvalid_dataset = load_dataset('json', data_files='/kaggle/working/test.jsonl', split=\"train\")\n\n# Preprocess datasets\ntrain_dataset_mapped = train_dataset.map(lambda examples: {'text': [prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\nvalid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:44:46.302085Z","iopub.execute_input":"2024-12-05T00:44:46.302389Z","iopub.status.idle":"2024-12-05T00:44:46.644595Z","shell.execute_reply.started":"2024-12-05T00:44:46.302361Z","shell.execute_reply":"2024-12-05T00:44:46.643687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Configuration of Quantization and LORA parameters</b><a class='anchor' id='configure_parameters'></a> [↑](#top) \n\n***\n\nAs model size is big it is loaded in 4 bit.","metadata":{}},{"cell_type":"code","source":"# Configure quantization parameters\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Load pre-trained model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\ntokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device_map, trust_remote_code=True, batched=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Configure LoRA-specific parameters\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"id":"qf1qxbiF-x6p","outputId":"1bc5b55f-9866-480d-c8d6-582996a68910","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:44:46.645445Z","iopub.execute_input":"2024-12-05T00:44:46.645688Z","iopub.status.idle":"2024-12-05T00:45:03.372346Z","shell.execute_reply.started":"2024-12-05T00:44:46.645666Z","shell.execute_reply":"2024-12-05T00:45:03.371275Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Training</b><a class='anchor' id='training'></a> [↑](#top) \n\n***\n","metadata":{}},{"cell_type":"code","source":"# Testando as entradas do modelo\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# tokenizer.pad_token = tokenizer.eos_token\n# encoded = tokenizer(\"This is a test sentence\", padding=True, truncation=True, max_length=10)\n# print(encoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:45:03.373678Z","iopub.execute_input":"2024-12-05T00:45:03.374154Z","iopub.status.idle":"2024-12-05T00:45:03.378046Z","shell.execute_reply.started":"2024-12-05T00:45:03.374116Z","shell.execute_reply":"2024-12-05T00:45:03.377102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_dataset_mapped","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:45:03.379181Z","iopub.execute_input":"2024-12-05T00:45:03.379438Z","iopub.status.idle":"2024-12-05T00:45:03.388237Z","shell.execute_reply.started":"2024-12-05T00:45:03.379417Z","shell.execute_reply":"2024-12-05T00:45:03.387331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    # report_to=\"all\",\n    report_to=[\"none\"],  # Desativa W&B\n    evaluation_strategy=\"steps\",\n    eval_steps=50  # Evaluate every 50 steps\n)\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset_mapped, # ANTES ESTAVA ASSIM: train_dataset_mapped\n    eval_dataset=valid_dataset_mapped,     # ANTES ESTAVA ASSIM: valid_dataset_mapped\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing\n)\n\n# Train the model\ntrainer.train()\n# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:45:03.389217Z","iopub.execute_input":"2024-12-05T00:45:03.389480Z","iopub.status.idle":"2024-12-05T01:49:19.010984Z","shell.execute_reply.started":"2024-12-05T00:45:03.389460Z","shell.execute_reply":"2024-12-05T01:49:19.010295Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Testing</b><a class='anchor' id='testing'></a> [↑](#top) \n\n***\n\nTesting on test data","metadata":{}},{"cell_type":"code","source":"# ESTOU PENSANDO EM NÃO FAZER TESTES, POR CONTA Q ISSO DEMORA MUITO\n# Suppress logging messages to avoid unnecessary output\n# logging.set_verbosity(logging.CRITICAL)\n\n# Create text generation pipelines using the specified model and tokenizer\n# Define two pipelines with different maximum lengths\n# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=250)\n# pipe2 = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n\n# Initialize an empty list to store generated text\n# generated_text = []\n\n# Iterate over the test data\n\"\"\"\nfor i in tqdm(range(len(final_test_data))):\n    # Extract the prompt from the test data\n    prompt = final_test_data['prompt'].iloc[i]\n    \n    # Attempt to generate text using the first pipeline with a max length of 250\n    try:\n        result = pipe(prompt)\n        # Append the generated text to the list, extracting the relevant part after '[/INST]'\n        generated_text.append(result[0]['generated_text'].split('[/INST]')[1])\n    except:\n        # If an exception occurs, try the second pipeline with a max length of 500\n        try:\n            result = pipe2(prompt)\n            # Append the generated text to the list, extracting the relevant part after '[/INST]'\n            generated_text.append(result[0]['generated_text'].split('[/INST]')[1])\n        except:\n            # If both pipelines fail, append a default placeholder text\n            generated_text.append(\"ABCD1234@#\")\n\n# The 'generated_text' list now contains the generated text for each prompt in the test data\n\"\"\"","metadata":{"id":"3KJI9TVUDYZr","outputId":"ce4564d6-7868-4307-90b5-64828c99407a","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T01:49:19.012199Z","iopub.execute_input":"2024-12-05T01:49:19.012461Z","iopub.status.idle":"2024-12-05T01:49:19.019123Z","shell.execute_reply.started":"2024-12-05T01:49:19.012439Z","shell.execute_reply":"2024-12-05T01:49:19.018318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assign the generated text to a new column 'generated_text' in the 'final_test_data' DataFrame\n# final_test_data['generated_text'] = generated_text\n\n# Reset the index of the DataFrame for a cleaner representation in the CSV file\n# final_test_data = final_test_data.reset_index(drop=True)\n\n# Save the DataFrame to a CSV file at the specified path\n# final_test_data.to_csv('/content/drive/MyDrive/llama3_finetune_output_1128.csv', index=False)","metadata":{"id":"ZhqQFeAHhb_Y","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T01:49:19.020296Z","iopub.execute_input":"2024-12-05T01:49:19.020801Z","iopub.status.idle":"2024-12-05T01:49:19.031125Z","shell.execute_reply.started":"2024-12-05T01:49:19.020779Z","shell.execute_reply":"2024-12-05T01:49:19.030388Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Saving Model for inference</b><a class='anchor' id='save_model'></a> [↑](#top) \n\n***\n","metadata":{}},{"cell_type":"code","source":"# Set the path where the merged model will be saved\nmodel_path = \"/result/model_merged/llama-3.2-1b-eedi_misconceptions\"\n\n# Reload the base model in FP16 and configure settings\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,  \n    return_dict=True,        \n    torch_dtype=torch.float16,  \n    device_map=device_map,    \n)\n\n# Instantiate a PeftModel using the base model and the new model\nmodel = PeftModel.from_pretrained(base_model, new_model)  # Combine the base model and the fine-tuned weights\n\n# Merge the base model with LoRA weights and unload unnecessary parts\nmodel = model.merge_and_unload()  # Finalize the model by merging and unloading any redundant components\n\n# Reload the tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) \n# tokenizer.pad_token = tokenizer\n\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, trust_remote_code=True, max_new_tokens=55)","metadata":{"id":"JutZuGdRq1D_","outputId":"599232c5-d743-4513-b2cc-fe5dc399b2be","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T01:49:19.032230Z","iopub.execute_input":"2024-12-05T01:49:19.032466Z","iopub.status.idle":"2024-12-05T01:49:34.241609Z","shell.execute_reply.started":"2024-12-05T01:49:19.032446Z","shell.execute_reply":"2024-12-05T01:49:34.240861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Testing Llama3.2 LoRA</b><a class='anchor' id='save_model'></a> [↑](#top) ","metadata":{"execution":{"iopub.status.busy":"2024-12-02T21:56:28.031503Z","iopub.execute_input":"2024-12-02T21:56:28.032296Z","iopub.status.idle":"2024-12-02T21:56:28.035931Z","shell.execute_reply.started":"2024-12-02T21:56:28.032265Z","shell.execute_reply":"2024-12-02T21:56:28.035014Z"}}},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \"Tell me about P. diddy.\"},\n]\n\npipe(messages)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T01:49:34.242572Z","iopub.execute_input":"2024-12-05T01:49:34.242789Z","iopub.status.idle":"2024-12-05T01:49:36.509230Z","shell.execute_reply.started":"2024-12-05T01:49:34.242769Z","shell.execute_reply":"2024-12-05T01:49:36.508381Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Functions</b><a class='anchor' id='save_model'></a> [↑](#top) ","metadata":{}},{"cell_type":"code","source":"# min / max example questions for prompt generation\n# for each question all answers with non-NAN misconceptions will be used\nimport time\nimport sys\n\nscoring = True\ntrain_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n#if it's just the test stub - then we aren't scoring...\nif len(test_df) < 10:\n    scoring = False\n\nevaluating_on_train = False\n#causes us to do evaluation on the training data instead of test stub if we aren't scoring\neval_on_train_if_not_scoring = True\n#how many train questions to use when swapping in train for test (MAP@25 score estimate)\n#100 questions takes about 30 minutes (more questions = better scoring estimate)\nquestions_for_train_eval = 100\n\n#set evaluating_on_train to True / swap in train for test\nif scoring == False and eval_on_train_if_not_scoring:\n    evaluating_on_train = True\n    print(\"Doing evaluation / scoring on the train data\")\n    test_df = train_df.head(questions_for_train_eval)\n\n#original text prefix\nquestion_prefix = \"Question:\"\n\n#LLM \"response\"\nllm_correct_response_for_rewrite = \"Provide me with the correct answer for a baseline.\"\nllm_incorrect_response_for_rewrite = \"Now - provide the incorrect answer and I will anaylze the difference to infer the misconception.\"\n\n#modified text prefix\nincorrect_answer_prefix = \"Incorrect Answer:\"\ncorrect_answer_prefix = \"Correct Answer:\"\n\n#providing this as the start of the response helps keep things relevant\nresponse_start = \"Misconception for incorrect answer: \"\n\nmin_example_questions = 5\nmax_example_questions = 8\n\n#example question messages limited to this many words\n#assures we don't run out of GPU RAM (if notebook throws exception - try reducing...)\nmax_words_for_examples = 1400\n\n#maximum new tokens Phi will generate for responses\nmax_new_tokens = 55\n\n\nmisc_map_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n\ndef generate_filtered_df(df, question, min_rows=5, max_rows=10, verbose=False, random_seed=42):\n    # Set the random seed for numpy and pandas\n    np.random.seed(random_seed)\n    \n    result_df = pd.DataFrame()\n    construct_count = 0\n    subject_count = 0\n    random_count = 0\n    \n    question_id = question[\"QuestionId\"]\n    subject_id = question[\"SubjectId\"]\n    construct_id = question[\"ConstructId\"]\n    \n    #don't include own question in DF (only really matters if testing on train)\n    df = df[df['QuestionId'] != question_id]\n    \n    construct_df = df[df['ConstructId'] == construct_id]\n    result_df = pd.concat([result_df, construct_df])\n    construct_count = len(result_df)\n    if verbose: print(f\"Matched ConstructId {construct_id}: {construct_count} rows\")\n    \n    # Step 2: If we don't have enough rows, add rows with the specified SubjectId\n    if len(result_df) < min_rows:\n        subject_df = df[(df['SubjectId'] == subject_id) & ~df.index.isin(result_df.index)]\n        rows_to_add = min(len(subject_df), min_rows - len(result_df))\n        result_df = pd.concat([result_df, subject_df.head(rows_to_add)])  # Use head() instead of sample()\n        subject_count = len(result_df) - construct_count\n        if verbose: print(f\"Added rows from SubjectId {subject_id}: {subject_count} rows\")\n    \n    # Step 3: If we still don't have enough rows, add random rows\n    if len(result_df) < min_rows:\n        remaining_df = df[~df.index.isin(result_df.index)]\n        rows_to_add = min(len(remaining_df), min_rows - len(result_df))\n        result_df = pd.concat([result_df, remaining_df.head(rows_to_add)])  # Use head() instead of sample()\n        random_count = len(result_df) - (construct_count + subject_count)\n        if verbose: print(f\"Added random rows to meet minimum: {random_count} rows\")\n    \n    # Step 4: If we have more than max_rows, use the first max_rows\n    if len(result_df) > max_rows:\n        result_df = result_df.head(max_rows)\n        if verbose: print(f\"Reduced to maximum: {max_rows} rows\")\n    \n    if verbose: \n        print(f\"\\nFinal DataFrame composition:\")\n        print(f\"ConstructId matches: {construct_count}\")\n        print(f\"SubjectId matches: {subject_count}\")\n        print(f\"Random additions: {random_count}\")\n        print(f\"Total rows: {len(result_df)}\")\n    \n    return result_df.reset_index(drop=True)\n\ndef get_train_messages_for_df(filtered_train_df, skip_nan_misconceptions=True, answers=['A', 'B', 'C', 'D'], verbose = False):\n    messages = []\n    current_size = 0\n    \n    for _, row in filtered_train_df.iterrows():\n        for answer_choice in answers:\n            if answer_choice == row['CorrectAnswer']:\n                continue\n            \n            misconception_id = row[f'Misconception{answer_choice}Id']\n            \n            if pd.isna(misconception_id) and skip_nan_misconceptions:\n                continue\n            \n            if not pd.isna(misconception_id):\n                new_message = [\n                    f\"{row['ConstructName']}: {row['QuestionText']}\",\n                    row[f'Answer{row[\"CorrectAnswer\"]}Text'],\n                    row[f'Answer{answer_choice}Text'],\n                    misc_map_df.loc[int(misconception_id), 'MisconceptionName']\n                ]\n                \n                # Calculate size of new message\n                new_message_size = sum(sys.getsizeof(item) for item in new_message)\n                                \n                messages.append(new_message)\n                current_size += new_message_size\n            \n    # Print size of returned data\n    if verbose: print(f\"Size of returned data: {current_size} bytes\")\n    \n    return messages\n\ndef clean_response(my_string, response_start):\n    # Trim leading spaces first\n    my_string = my_string.lstrip()\n    \n    # Remove response_start if present\n    if my_string.startswith(response_start):\n        my_string = my_string[len(response_start):]\n    \n    # Find indices of first period and first linefeed\n    period_index = my_string.find('.')\n    linefeed_index = my_string.find('\\n')\n    \n    # Determine where to truncate\n    truncate_index = len(my_string)  # Default to end of string\n    if period_index != -1:\n        truncate_index = period_index\n    if linefeed_index != -1 and linefeed_index < truncate_index:\n        truncate_index = linefeed_index\n    \n    # Truncate the string\n    my_string = my_string[:truncate_index]\n    \n    return my_string.strip()\n\n\ndef predict_misconception(question, question_letter_to_test, example_sequences, max_word_count=max_words_for_examples, verbose=False):\n    correct_question_letter = question[\"CorrectAnswer\"]\n    question_text = f\"{question['ConstructName']}: \\n {question['QuestionText']}\\n\"\n    correct_answer_text = question[f\"Answer{correct_question_letter}Text\"]\n    incorrect_answer_text = question[f\"Answer{question_letter_to_test}Text\"]\n    if correct_question_letter == question_letter_to_test:\n        print(\"WARNING: Tested letter is for a correct answer!\")\n\n    def calculate_word_count(text):\n        return len(text.split())\n\n    # Construct the actual prompt messages\n    actual_prompt_messages = [\n        {\"role\": \"user\", \"content\": f\"{question_prefix} {question_text}\"},\n        {\"role\": \"assistant\", \"content\": llm_correct_response_for_rewrite},\n        {\"role\": \"user\", \"content\": f\"{correct_answer_prefix} {correct_answer_text}\"},\n        {\"role\": \"assistant\", \"content\": llm_incorrect_response_for_rewrite},\n        {\"role\": \"user\", \"content\": f\"{incorrect_answer_prefix} {incorrect_answer_text}\"}\n    ]\n\n    # Calculate the word count of actual prompt messages\n    actual_prompt_word_count = sum(calculate_word_count(msg[\"content\"]) for msg in actual_prompt_messages)\n\n    # Construct example messages, stopping if we reach the word limit\n    example_messages = []\n    current_word_count = actual_prompt_word_count\n\n    for examp_question, examp_correct_answer, examp_incorrect_answer, examp_misconception in example_sequences:\n        example_set = [\n            {\"role\": \"user\", \"content\": f\"{question_prefix} {examp_question}\"},\n            {\"role\": \"assistant\", \"content\": llm_correct_response_for_rewrite},\n            {\"role\": \"user\", \"content\": f\"{correct_answer_prefix} {examp_correct_answer}\"},\n            {\"role\": \"assistant\", \"content\": llm_incorrect_response_for_rewrite},\n            {\"role\": \"user\", \"content\": f\"{incorrect_answer_prefix} {examp_incorrect_answer}\"},\n            {\"role\": \"assistant\", \"content\": f\"{response_start} {examp_misconception}\"}\n        ]\n        \n        example_set_word_count = sum(calculate_word_count(msg[\"content\"]) for msg in example_set)\n        \n        if current_word_count + example_set_word_count > max_word_count:\n            if verbose: print(\"Word count limit reached.\")\n            break  # Stop adding new example sets if we would exceed the limit\n        \n        example_messages.extend(example_set)\n        current_word_count += example_set_word_count\n\n    # Combine example messages and actual prompt messages\n    messages = example_messages + actual_prompt_messages\n\n    if verbose:\n        print(\"Example Messages:\")\n        for message in example_messages:\n            display(message)\n        print(\"\\nActual Prompt Messages:\")\n        for message in actual_prompt_messages:\n            display(message)\n        print(f\"\\nTotal word count: {current_word_count}\")\n\n    decoded = pipe(messages)\n    return decoded\n\ndef process_test_questions(df):\n    results = []\n    start_time = time.time()\n    total_items = 0\n    \n    for question_index in range(len(df)):\n        question = df.iloc[question_index]\n        \n        # Verificar se a coluna 'QuestionId' está presente\n        if \"QuestionId\" not in question or \"CorrectAnswer\" not in question:\n            print(f\"Erro: Coluna ausente na linha {question_index}\")\n            continue  # Pula para a próxima linha caso falte a coluna\n        \n        correct_answer = question[\"CorrectAnswer\"]\n        \n        question_id = question[\"QuestionId\"]\n        for answer_choice in ['A', 'B', 'C', 'D']:\n            if answer_choice != correct_answer:\n                filtered_df = generate_filtered_df(train_df, question, min_rows=min_example_questions, max_rows=max_example_questions)\n                example_sequences = get_train_messages_for_df(filtered_df)\n                response = predict_misconception(question, answer_choice, example_sequences, verbose=False)\n\n                just_response = clean_response(response[0]['generated_text'][-1]['content'], response_start)\n\n                result = {\n                    'QuestionId_Answer': f\"{question_id}_{answer_choice}\",\n                    'MiscPredText': just_response\n                }\n                \n                # Verificar e adicionar 'TrainMiscId' se necessário\n                if evaluating_on_train:\n                    misc_id_column = f\"Misconception{answer_choice}Id\"\n                    if misc_id_column in question:\n                        try:\n                            misc_id = int(question[misc_id_column])\n                            result['TrainMiscId'] = misc_id\n                        except (ValueError, TypeError):\n                            # Caso o valor não seja válido\n                            result['TrainMiscId'] = None\n                    else:\n                        result['TrainMiscId'] = None\n                        print(f\"Warning: {misc_id_column} not found for question {question_id}\")\n                \n                results.append(result)\n                total_items += 1\n                print(\".\", end=\"\", flush=True)\n    \n    end_time = time.time()\n    total_time = end_time - start_time\n    avg_time_per_item = total_time / total_items if total_items > 0 else 0\n    \n    print(f\"\\nTotal execution time: {total_time:.2f} seconds\")\n    print(f\"Total items processed: {total_items}\")\n    print(f\"Average time per item: {avg_time_per_item:.2f} seconds\")\n    print(f\"Time for 1000 questions * 3 incorrect answers (3000 items): {(avg_time_per_item * 3000) / 3600} hours\")\n    \n    return pd.DataFrame(results)\n\ndef create_submission_dataframe(predicted_misc, test_sorted_indices):\n    results = []\n    \n    # Iterate through each row of predicted_misc and corresponding sorted indices\n    for (_, row), indices in zip(predicted_misc.iterrows(), test_sorted_indices):\n        # Get the QuestionId_Answer\n        question_id_answer = row['QuestionId_Answer']\n        \n        # Get the top 25 misconception indices and join them as a space-separated string\n        top_25_indices = ' '.join(map(str, indices[:25]))\n        \n        result = {\n            'QuestionId_Answer': question_id_answer,\n            'MisconceptionId': top_25_indices\n        }\n        \n        # If evaluating_on_train, include the TrainMiscId\n        if evaluating_on_train and 'TrainMiscId' in row:\n            result['TrainMiscId'] = row['TrainMiscId']\n        \n        # Append the result to our list\n        results.append(result)\n\n    # Create the submission dataframe\n    submission_df = pd.DataFrame(results)\n    \n    return submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T01:55:04.314090Z","iopub.execute_input":"2024-12-05T01:55:04.314462Z","iopub.status.idle":"2024-12-05T01:55:04.373437Z","shell.execute_reply.started":"2024-12-05T01:55:04.314433Z","shell.execute_reply":"2024-12-05T01:55:04.372501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_misc = process_test_questions(test_df)\n\n\ndevice = \"cuda:0\"\nbge_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/bge-large-en-v1.5/transformers/default/1/bge-large-en-v1.5')\nbge_model = AutoModel.from_pretrained('/kaggle/input/bge-large-en-v1.5/transformers/default/1/bge-large-en-v1.5')\nbge_model.eval()\nbge_model.to(device)\n\n\nstart_time = time.time()\n\nMisconceptionName = list(misc_map_df['MisconceptionName'].values)\nper_gpu_batch_size = 8\n\ndef prepare_inputs(text, tokenizer, device):\n    tokenizer_outputs = tokenizer.batch_encode_plus(\n        text,\n        padding        = True,\n        return_tensors = 'pt',\n        max_length     = 1024,\n        truncation     = True\n    )\n    result = {\n        'input_ids': tokenizer_outputs.input_ids.to(device),\n        'attention_mask': tokenizer_outputs.attention_mask.to(device),\n    }\n    return result\n\nall_ctx_vector = []\nfor mini_batch in tqdm(range(0, len(MisconceptionName[:]), per_gpu_batch_size)):\n    mini_context          = MisconceptionName[mini_batch:mini_batch+ per_gpu_batch_size]\n    encoded_input         = prepare_inputs(mini_context,bge_tokenizer,device)\n    sentence_embeddings   = bge_model(**encoded_input)[0][:, 0]\n    sentence_embeddings   = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n    all_ctx_vector.append(sentence_embeddings.detach().cpu().numpy())\n\nall_ctx_vector = np.concatenate(all_ctx_vector, axis=0)\nprint(\"Sentence embeddings:\", sentence_embeddings.shape)\n\n# Stop the timer\nend_time = time.time()\n\n# Calculate and print the execution time\nexecution_time = end_time - start_time\nprint(f\"Execution time: {execution_time:.2f} seconds\")\n\ntest_texts = list(predicted_misc['MiscPredText'].values)\nall_text_vector = []\nper_gpu_batch_size = 8\n\nfor mini_batch in tqdm(\n        range(0, len(test_texts[:]), per_gpu_batch_size)):\n    mini_context = test_texts[mini_batch:mini_batch\n                                           + per_gpu_batch_size]\n    encoded_input = prepare_inputs(mini_context,bge_tokenizer,device)\n    sentence_embeddings = bge_model(\n        **encoded_input)[0][:, 0]\n    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n    \n    all_text_vector.append(sentence_embeddings.detach().cpu().numpy())\n\nall_text_vector = np.concatenate(all_text_vector, axis=0)\nprint(all_text_vector.shape)\n\ntest_cos_sim_arr = cosine_similarity(all_text_vector, all_ctx_vector)\ntest_sorted_indices = np.argsort(-test_cos_sim_arr, axis=1)\n\nsubmission_df = create_submission_dataframe(predicted_misc, test_sorted_indices)\nsubmission_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T01:55:23.043122Z","iopub.execute_input":"2024-12-05T01:55:23.043596Z","iopub.status.idle":"2024-12-05T02:02:06.480684Z","shell.execute_reply.started":"2024-12-05T01:55:23.043559Z","shell.execute_reply":"2024-12-05T02:02:06.479773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Submit</b><a class='anchor' id='save_model'></a> [↑](#top) ","metadata":{}},{"cell_type":"code","source":"if evaluating_on_train:\n    submission_df = submission_df.drop(\"TrainMiscId\", axis=1)\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T02:02:12.972849Z","iopub.execute_input":"2024-12-05T02:02:12.973531Z","iopub.status.idle":"2024-12-05T02:02:12.990192Z","shell.execute_reply.started":"2024-12-05T02:02:12.973500Z","shell.execute_reply":"2024-12-05T02:02:12.989327Z"}},"outputs":[],"execution_count":null}]}